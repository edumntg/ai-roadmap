{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import torch\n",
    "from gym.envs.registration import register\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "from collections import deque\n",
    "import random\n",
    "import tqdm\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaterTankEnv(gym.Env):\n",
    "\n",
    "    @staticmethod\n",
    "    def gen_params(A = 20.0, a = 2.0, b = 5.0) -> dict:\n",
    "        \"\"\"Returns a ``dict`` containing the physical parameters such as gravitational force and torque or speed limits.\"\"\"\n",
    "\n",
    "        td = {\n",
    "                \"max_voltage\": 120.0,\n",
    "                \"min_voltage\": 0.0,\n",
    "                \"max_H\": 30.0,\n",
    "                \"min_H\": 1e-6,\n",
    "                \"setpoint\": 10.0, # setpoint\n",
    "                \"A\": A,\n",
    "                \"a\": a,\n",
    "                \"b\": b,\n",
    "                \"dt\": 0.05,\n",
    "                \"P_hat\": 0.0,\n",
    "                \"eps\": 1e-3,\n",
    "                \"tf\": 100\n",
    "        }\n",
    "        return td\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        params = WaterTankEnv.gen_params()\n",
    "        self.params = params\n",
    "\n",
    "        # Create observation space. Pump voltage\n",
    "        self.observation_space = spaces.Box(\n",
    "            low = np.array([params[\"min_H\"]]),\n",
    "            high = np.array([params[\"max_H\"]])\n",
    "        )\n",
    "\n",
    "        # Action spaces (number of actions)\n",
    "        self.action_space = spaces.Box(\n",
    "            low = params[\"min_voltage\"],\n",
    "            high = params[\"max_voltage\"]\n",
    "        ) # possible voltage values\n",
    "\n",
    "        self._instantiate_initial_values()\n",
    "    \n",
    "    def _instantiate_initial_values(self):\n",
    "        # Instantiate all state variables and independent variables\n",
    "        if self.params is None:\n",
    "            self.params = WaterTankEnv.gen_params()\n",
    "\n",
    "        # Instantiate state variable\n",
    "        self.H = np.random.uniform(\n",
    "            low = self.params[\"min_H\"],\n",
    "            high = self.params[\"max_H\"]\n",
    "        )\n",
    "\n",
    "        # Instantiate voltage\n",
    "        self.v = np.random.uniform(\n",
    "            low = self.params[\"min_voltage\"],\n",
    "            high = self.params[\"max_voltage\"]\n",
    "        )\n",
    "\n",
    "        # Compute input flow\n",
    "        self.P = self.params[\"b\"]*self.v\n",
    "        # Compute output flow\n",
    "        self.Q = self.params[\"a\"]*np.sqrt(self.H)\n",
    "\n",
    "        self.state = np.array([self.H], dtype = np.float32) # our state\n",
    "\n",
    "        self.t = 0\n",
    "\n",
    "        return self.state\n",
    "\n",
    "    def reset(self):\n",
    "        return self._instantiate_initial_values() # state, info\n",
    "    \n",
    "    def step(self, action: torch.Tensor):\n",
    "        # Update the new voltage value\n",
    "        self.V = action\n",
    "        \n",
    "        # Clamp value\n",
    "        self.V = np.clip(self.V, self.params[\"min_voltage\"], self.params[\"max_voltage\"])\n",
    "\n",
    "        # Compute new input flow\n",
    "        self.P = self.V * self.params[\"b\"]\n",
    "\n",
    "        # Compute new output flow\n",
    "        self.Q = np.sqrt(self.H)*self.params[\"a\"]\n",
    "\n",
    "        # Get old water level\n",
    "        H = self.H\n",
    "\n",
    "        # Compute new H\n",
    "        new_H = H + self.params[\"dt\"]*(self.P - self.Q)/self.params[\"A\"]\n",
    "\n",
    "        # Update water level\n",
    "        self.H = new_H\n",
    "\n",
    "        # Clamp\n",
    "        self.H = np.clip(self.H, self.params[\"min_H\"], self.params[\"max_H\"])\n",
    "\n",
    "        # Compute cost\n",
    "        cost = np.abs(self.H - self.params[\"setpoint\"]) / self.params[\"setpoint\"]\n",
    "\n",
    "        # Compute reward\n",
    "        reward = -cost\n",
    "\n",
    "        # Increase time\n",
    "        self.t += self.params[\"dt\"]\n",
    "\n",
    "        # Check if simulation is terminated\n",
    "        terminated = np.abs(self.H - self.params[\"setpoint\"]) <= self.params[\"eps\"] or self.t >= self.params[\"tf\"]\n",
    "\n",
    "        # Return\n",
    "        return np.array([self.H], dtype = np.float32), reward, terminated, {} # obs, reward, terminated, truncated, info                    \n",
    "\n",
    "    def render(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\gym\\spaces\\box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[6.447536]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = WaterTankEnv()\n",
    "env.reset()\n",
    "action = torch.tensor([20], dtype = torch.float32)\n",
    "observation, reward, done, info = env.step(action)\n",
    "observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 Score: [-3982.5037] Mean: [-1.9902567]\n",
      "Episode 2 Score: [-3971.5444] Mean: [-1.9847798]\n",
      "Episode 3 Score: [-3962.839] Mean: [-1.9804293]\n",
      "Episode 4 Score: [-3970.301] Mean: [-1.9841584]\n",
      "Episode 5 Score: [-3979.2776] Mean: [-1.9886445]\n",
      "Episode 6 Score: [-3991.323] Mean: [-1.9946642]\n",
      "Episode 7 Score: [-3998.108] Mean: [-1.998055]\n",
      "Episode 8 Score: [-4001.061] Mean: [-1.9995308]\n",
      "Episode 9 Score: [-4000.6826] Mean: [-1.9993416]\n",
      "Episode 10 Score: [-3997.4282] Mean: [-1.9977152]\n"
     ]
    }
   ],
   "source": [
    "episodes = 10\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    episode_len = 0\n",
    "\n",
    "    while not done:\n",
    "        episode_len += 1\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "    print(\"Episode {} Score: {} Mean: {}\".format(episode, score, score / episode_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1612], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = env.observation_space.shape[0]\n",
    "output_dim = env.action_space.shape[0]\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(input_dim, 64),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(64, output_dim),\n",
    ")\n",
    "\n",
    "net(torch.tensor([5], dtype = torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodes 50, steps per episode 100\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define simulation time\n",
    "tf = 5 # in seconds\n",
    "N = int(math.ceil(tf/env.params[\"dt\"]))\n",
    "\n",
    "# Define training parameters\n",
    "num_episodes = 50 \n",
    "gamma = 0.99\n",
    "max_iters_per_episode = N # equivalent to tf seconds\n",
    "n_time_steps = N\n",
    "\n",
    "rewards = []\n",
    "water_levels = []\n",
    "input_flows = []\n",
    "\n",
    "print(f\"Episodes {num_episodes}, steps per episode {max_iters_per_episode}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1707], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test model and env\n",
    "state = env.reset()\n",
    "state = torch.tensor(state, dtype = torch.float32)\n",
    "q_values = net(state)\n",
    "q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), None and long or byte Variables are valid indices (got float)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 24\u001b[0m\n\u001b[0;32m     20\u001b[0m next_state_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(next_state, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     22\u001b[0m target_q_value \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(net(next_state_tensor))\n\u001b[1;32m---> 24\u001b[0m loss \u001b[38;5;241m=\u001b[39m  criterion(\u001b[43mq_values\u001b[49m\u001b[43m[\u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m, target_q_value)\n\u001b[0;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     27\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), None and long or byte Variables are valid indices (got float)"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "pbar = tqdm.tqdm(range(num_episodes))\n",
    "for episode in pbar:\n",
    "    #state = torch.tensor(env.step(0)[0], dtype=torch.float32)\n",
    "    state = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32)\n",
    "    total_reward = 0\n",
    "\n",
    "    done = False\n",
    "    episode_len = 0\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        q_values = net(state) # get model values\n",
    "\n",
    "        action = torch.max(q_values).item()\n",
    "                \n",
    "        next_state, reward, done, info = env.step(action) # Compute next state\n",
    "        \n",
    "        next_state_tensor = torch.tensor(next_state, dtype=torch.float32)\n",
    "        \n",
    "        target_q_value = reward + gamma * torch.max(net(next_state_tensor))\n",
    "        \n",
    "        loss =  criterion(q_values[action], target_q_value)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        state = next_state_tensor\n",
    "        total_reward += reward\n",
    "\n",
    "    print(f'Episode {episode + 1} - Total Reward: {total_reward}')\n",
    "    rewards.append(total_reward)\n",
    "    #water_levels.append(mean_episode_level)\n",
    "    #input_flows.append(mean_episode_inflow)\n",
    "\n",
    "#fig, axes = plt.subplots(nrows = 1, ncols = 3)\n",
    "plt.figure(1)\n",
    "plt.plot(np.arange(len(rewards))*env.dt, rewards)\n",
    "#axes[1].plot(np.arange(len(water_levels))*env.dt, water_levels)\n",
    "#axes[2].plot(np.arange(len(input_flows))*env.dt, input_flows)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
