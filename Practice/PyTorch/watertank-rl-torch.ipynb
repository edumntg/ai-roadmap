{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import gym\n",
    "import random\n",
    "import torch\n",
    "\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\gym\\envs\\registration.py:555: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "states = env.observation_space.shape[0]\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 Score: 19.0\n",
      "Episode 2 Score: 15.0\n",
      "Episode 3 Score: 49.0\n",
      "Episode 4 Score: 28.0\n",
      "Episode 5 Score: 14.0\n",
      "Episode 6 Score: 18.0\n",
      "Episode 7 Score: 19.0\n",
      "Episode 8 Score: 12.0\n",
      "Episode 9 Score: 12.0\n",
      "Episode 10 Score: 16.0\n",
      "Episode 11 Score: 16.0\n",
      "Episode 12 Score: 11.0\n",
      "Episode 13 Score: 10.0\n",
      "Episode 14 Score: 17.0\n",
      "Episode 15 Score: 11.0\n",
      "Episode 16 Score: 18.0\n",
      "Episode 17 Score: 45.0\n",
      "Episode 18 Score: 18.0\n",
      "Episode 19 Score: 9.0\n",
      "Episode 20 Score: 26.0\n",
      "Episode 21 Score: 16.0\n",
      "Episode 22 Score: 12.0\n",
      "Episode 23 Score: 15.0\n",
      "Episode 24 Score: 11.0\n",
      "Episode 25 Score: 10.0\n",
      "Episode 26 Score: 23.0\n",
      "Episode 27 Score: 13.0\n",
      "Episode 28 Score: 17.0\n",
      "Episode 29 Score: 14.0\n",
      "Episode 30 Score: 25.0\n",
      "Episode 31 Score: 9.0\n",
      "Episode 32 Score: 15.0\n",
      "Episode 33 Score: 31.0\n",
      "Episode 34 Score: 52.0\n",
      "Episode 35 Score: 12.0\n",
      "Episode 36 Score: 19.0\n",
      "Episode 37 Score: 17.0\n",
      "Episode 38 Score: 53.0\n",
      "Episode 39 Score: 21.0\n",
      "Episode 40 Score: 23.0\n",
      "Episode 41 Score: 16.0\n",
      "Episode 42 Score: 24.0\n",
      "Episode 43 Score: 14.0\n",
      "Episode 44 Score: 31.0\n",
      "Episode 45 Score: 24.0\n",
      "Episode 46 Score: 18.0\n",
      "Episode 47 Score: 22.0\n",
      "Episode 48 Score: 50.0\n",
      "Episode 49 Score: 25.0\n",
      "Episode 50 Score: 12.0\n",
      "Episode 51 Score: 34.0\n",
      "Episode 52 Score: 17.0\n",
      "Episode 53 Score: 15.0\n",
      "Episode 54 Score: 13.0\n",
      "Episode 55 Score: 13.0\n",
      "Episode 56 Score: 16.0\n",
      "Episode 57 Score: 13.0\n",
      "Episode 58 Score: 38.0\n",
      "Episode 59 Score: 21.0\n",
      "Episode 60 Score: 24.0\n",
      "Episode 61 Score: 19.0\n",
      "Episode 62 Score: 21.0\n",
      "Episode 63 Score: 12.0\n",
      "Episode 64 Score: 12.0\n",
      "Episode 65 Score: 31.0\n",
      "Episode 66 Score: 11.0\n",
      "Episode 67 Score: 15.0\n",
      "Episode 68 Score: 43.0\n",
      "Episode 69 Score: 20.0\n",
      "Episode 70 Score: 30.0\n",
      "Episode 71 Score: 38.0\n",
      "Episode 72 Score: 19.0\n",
      "Episode 73 Score: 15.0\n",
      "Episode 74 Score: 11.0\n",
      "Episode 75 Score: 30.0\n",
      "Episode 76 Score: 25.0\n",
      "Episode 77 Score: 39.0\n",
      "Episode 78 Score: 14.0\n",
      "Episode 79 Score: 19.0\n",
      "Episode 80 Score: 17.0\n",
      "Episode 81 Score: 24.0\n",
      "Episode 82 Score: 21.0\n",
      "Episode 83 Score: 17.0\n",
      "Episode 84 Score: 18.0\n",
      "Episode 85 Score: 14.0\n",
      "Episode 86 Score: 11.0\n",
      "Episode 87 Score: 35.0\n",
      "Episode 88 Score: 19.0\n",
      "Episode 89 Score: 14.0\n",
      "Episode 90 Score: 26.0\n",
      "Episode 91 Score: 14.0\n",
      "Episode 92 Score: 30.0\n",
      "Episode 93 Score: 15.0\n",
      "Episode 94 Score: 13.0\n",
      "Episode 95 Score: 11.0\n",
      "Episode 96 Score: 34.0\n",
      "Episode 97 Score: 21.0\n",
      "Episode 98 Score: 12.0\n",
      "Episode 99 Score: 13.0\n",
      "Episode 100 Score: 48.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:211: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"CartPole-v0\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "episodes = 100\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = random.choice([0,1])\n",
    "        n_state, reward, done, truncated,info = env.step(action)\n",
    "        score += reward\n",
    "    print(\"Episode {} Score: {}\".format(episode, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the WaterTank environment\n",
    "class WaterTank(Env):\n",
    "    def __init__(self, setpoint = 10.0, dt = 1e-3):\n",
    "        \"\"\"\n",
    "        The tank is governed by the following first-order differential equation\n",
    "        \n",
    "        A(dH/dt) = bV - a*sqrt(H)\n",
    "\n",
    "        The states of the system are: H\n",
    "        The inputs of the system are: v\n",
    "        The outputs of the system are: H\n",
    "\n",
    "        H:  Height of water\n",
    "        V: voltage applied to the pump\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Possible actions\n",
    "        self.action_space = Box(\n",
    "            low = np.array([0.0]),\n",
    "            high = np.array([120.0])\n",
    "        ) # voltage values\n",
    "        \n",
    "        # Water level temperature array\n",
    "        self.observation_space = Box(low = np.array([1e-6]), high = np.array([30.0]))\n",
    "\n",
    "        # Tank parameters\n",
    "        self.H0 = 100 + np.random.uniform(-100, 100) # initial level\n",
    "        self.A = 20 # cross sectional area of the tank\n",
    "\n",
    "        # Initialize current water level\n",
    "        self.H = self.H0 # meters\n",
    "\n",
    "        # Set desired level\n",
    "        self.setpoint = setpoint\n",
    "\n",
    "        # Input valve parameters\n",
    "        self.b = 5 # Constant related to the flow rate into the tank\n",
    "        self.V = 5 + np.random.uniform(-5, 10) # Initial voltage of the pump\n",
    "        self.P = self.V*self.b\n",
    "\n",
    "        # Output valve parameters\n",
    "        self.a = 2 # Constant related to the flow rate out of the tank\n",
    "        self.Q = self.a*math.sqrt(self.H) # Output flow depends of pressure which depends of water height\n",
    "\n",
    "        self.dt = dt # each iteration is a 10ms time-step\n",
    "\n",
    "        # Initial water level, inflow and outflow\n",
    "        self.state = self.H\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        # action is the voltage\n",
    "\n",
    "        # Update inflow\n",
    "        self.P = self.b*self.V\n",
    "\n",
    "        # Output flow\n",
    "        self.Q = self.a*math.sqrt(self.H)\n",
    "\n",
    "        # Update tank level using trapezoidal method\n",
    "        Hold = self.H\n",
    "        Hnew = Hold + (self.dt/2.0)*((self.P - self.Q)/self.A)\n",
    "        #self.H += ((self.P - self.Q)/self.A) * self.dt # Trapezoidal method\n",
    "        self.H = Hnew\n",
    "        if self.H <= 0:\n",
    "            self.H = 1e-6\n",
    "\n",
    "        # Compute reward as the negative abs difference between current level and setpoint\n",
    "        \n",
    "        reward = -abs(self.H - self.setpoint)  # Negative absolute difference as reward\n",
    "\n",
    "        done = abs(self.H - self.setpoint) < 3\n",
    "\n",
    "        info = {}\n",
    "        self.state = np.array([self.H, self.V, self.Q])\n",
    "        return self.state, reward, done, False, info\n",
    "    \n",
    "    def render(self):\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        # Initialize current water level\n",
    "        self.H0 = 100 + np.random.uniform(-100, 100) # initial level\n",
    "        self.H = self.H0 # meters\n",
    "\n",
    "        # Input valve parameters\n",
    "        self.V = 5 + np.random.uniform(-5, 10) # Initial voltage of the pump\n",
    "        self.P = self.V*self.b\n",
    "\n",
    "        # Output valve parameters\n",
    "        self.Q = self.a*math.sqrt(self.H) # Output flow depends of pressure which depends of water height\n",
    "\n",
    "        reward = -abs(self.H - self.setpoint)  # Negative absolute difference as reward\n",
    "\n",
    "        self.state = np.array([self.H, self.V, self.Q])\n",
    "\n",
    "        return self.state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Box shape is inferred from low and high, expect their types to be np.ndarray, an integer or a float, actual type low: <class 'torch.Tensor'>, high: <class 'torch.Tensor'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mWaterTank\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 19\u001b[0m, in \u001b[0;36mWaterTank.__init__\u001b[1;34m(self, setpoint, dt)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03mThe tank is governed by the following first-order differential equation\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m \n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Possible actions\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space \u001b[38;5;241m=\u001b[39m \u001b[43mBox\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1e-6\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhigh\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m30.0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Water level temperature array\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space \u001b[38;5;241m=\u001b[39m Box(low \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m1e-6\u001b[39m]), high \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m200\u001b[39m]))\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\gym\\spaces\\box.py:99\u001b[0m, in \u001b[0;36mBox.__init__\u001b[1;34m(self, low, high, shape, dtype, seed)\u001b[0m\n\u001b[0;32m     97\u001b[0m     shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m,)\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    100\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBox shape is inferred from low and high, expect their types to be np.ndarray, an integer or a float, actual type low: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(low)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, high: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(high)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    101\u001b[0m     )\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# Capture the boundedness information before replacing np.inf with get_inf\u001b[39;00m\n\u001b[0;32m    104\u001b[0m _low \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfull(shape, low, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m is_float_integer(low) \u001b[38;5;28;01melse\u001b[39;00m low\n",
      "\u001b[1;31mValueError\u001b[0m: Box shape is inferred from low and high, expect their types to be np.ndarray, an integer or a float, actual type low: <class 'torch.Tensor'>, high: <class 'torch.Tensor'>"
     ]
    }
   ],
   "source": [
    "env = WaterTank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 Score: -83723.1303793332\n",
      "Episode 2 Score: -46359180337.74294\n",
      "Episode 3 Score: -358306725.11723787\n",
      "Episode 4 Score: -7064363.9681306025\n"
     ]
    }
   ],
   "source": [
    "episodes = 10\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, truncated,info = env.step(action)\n",
    "        score += reward\n",
    "    print(\"Episode {} Score: {}\".format(episode, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Deep Q-Network (DQN) model\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, layers = [32, 64, 128, 256]):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        # Create input layer\n",
    "        self.input = nn.Linear(input_dim, layers[0])\n",
    "        self.output = nn.Linear(layers[-1], output_dim)\n",
    "\n",
    "        # Hidden layers\n",
    "        self.layers = []\n",
    "        for i in range(len(layers)-1):\n",
    "            self.layers.append(\n",
    "                nn.Linear(layers[i], layers[i+1])\n",
    "            )\n",
    "\n",
    "        #self.hidden = nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = torch.relu(self.input(x))\n",
    "\n",
    "        #x = torch.relu(self.hidden(x))\n",
    "        for layer in self.layers:\n",
    "            x = torch.relu(layer(x))\n",
    "\n",
    "        return self.output(x) # linear output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0110,  0.0112,  0.0381,  0.0379, -0.1070], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states = env.observation_space.shape[0]\n",
    "actions = env.action_space.n\n",
    "\n",
    "model = DQN(states, actions)\n",
    "model(torch.tensor([5], dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodes 50, steps per episode 5000\n"
     ]
    }
   ],
   "source": [
    "# Initialize the environment and DQN model\n",
    "env = WaterTank(\n",
    "    setpoint = 3\n",
    ")\n",
    "state_dim = env.state_variables_dim\n",
    "action_dim = env.actions_dim\n",
    "model = DQN(state_dim, action_dim, [128, 64, 32]).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define simulation time\n",
    "tf = 5 # in seconds\n",
    "N = int(math.ceil(tf/env.dt))\n",
    "\n",
    "# Define training parameters\n",
    "num_episodes = 50 \n",
    "gamma = 0.99\n",
    "max_iters_per_episode = N # equivalent to tf seconds\n",
    "n_time_steps = N\n",
    "\n",
    "rewards = []\n",
    "water_levels = []\n",
    "input_flows = []\n",
    "\n",
    "print(f\"Episodes {num_episodes}, steps per episode {max_iters_per_episode}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1315, 0.0390], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test model and env\n",
    "state, reward = env.reset()\n",
    "state = torch.tensor(state, dtype = torch.float32, device = device)\n",
    "q_values = model(state)\n",
    "q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 - Total Reward: -153105.36849028646\n",
      "Episode 2 - Total Reward: -2164770.4303629817\n",
      "Episode 3 - Total Reward: -2611235.5538996058\n",
      "Episode 4 - Total Reward: -2611235.5538996058\n",
      "Episode 5 - Total Reward: -2293736.718196942\n",
      "Episode 6 - Total Reward: -14900.338300455654\n",
      "Episode 7 - Total Reward: -14900.338300455654\n",
      "Episode 8 - Total Reward: -14900.338300455654\n",
      "Episode 9 - Total Reward: -14900.338300455654\n",
      "Episode 10 - Total Reward: -14900.338300455654\n",
      "Episode 11 - Total Reward: -14900.338300455654\n",
      "Episode 12 - Total Reward: -14900.338300455654\n",
      "Episode 13 - Total Reward: -14900.338300455654\n",
      "Episode 14 - Total Reward: -14900.338300455654\n",
      "Episode 15 - Total Reward: -14900.338300455654\n",
      "Episode 16 - Total Reward: -14900.338300455654\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 28\u001b[0m\n\u001b[0;32m     24\u001b[0m next_state, reward \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action) \u001b[38;5;66;03m# Compute next state\u001b[39;00m\n\u001b[0;32m     26\u001b[0m next_state_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(next_state, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device \u001b[38;5;241m=\u001b[39m device)\n\u001b[1;32m---> 28\u001b[0m target_q_value \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_state_tensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m loss \u001b[38;5;241m=\u001b[39m  criterion(q_values[action], target_q_value)\n\u001b[0;32m     32\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for episode in range(num_episodes):\n",
    "    #state = torch.tensor(env.step(0)[0], dtype=torch.float32)\n",
    "    state, reward = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device = device)\n",
    "    total_reward = 0\n",
    "\n",
    "    #mean_episode_level = 0\n",
    "    #mean_episode_inflow = 0\n",
    "    #mean_episode_outflow = 0\n",
    "\n",
    "    for i in range(n_time_steps):\n",
    "    #while not done:\n",
    "        ti = (i-1)*env.dt # current time\n",
    "\n",
    "        q_values= model(state) # get model values\n",
    "        \n",
    "        #valve = torch.relu(q_values[-1]).item() # Valve voltage\n",
    "\n",
    "        #q_values = q_values[:2] # Actions\n",
    "        #q_values = torch.softmax(q_values, dim = 0) # Compute actions probabilities\n",
    "        action = torch.argmax(q_values).item() # Pick action with highest prob\n",
    "        \n",
    "        next_state, reward = env.step(action) # Compute next state\n",
    "        \n",
    "        next_state_tensor = torch.tensor(next_state, dtype=torch.float32, device = device)\n",
    "        \n",
    "        target_q_value = reward + gamma * torch.max(model(next_state_tensor))\n",
    "        \n",
    "        loss =  criterion(q_values[action], target_q_value)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        state = next_state_tensor\n",
    "        total_reward += reward\n",
    "\n",
    "        #mean_episode_level += state[0]\n",
    "        #mean_episode_inflow += state[1]\n",
    "        #mean_episode_outflow += state[2]\n",
    "\n",
    "    #mean_episode_level /= max_iters_per_episode\n",
    "    #mean_episode_inflow /= max_iters_per_episode\n",
    "    #mean_episode_outflow /= max_iters_per_episode\n",
    "\n",
    "    print(f'Episode {episode + 1} - Total Reward: {total_reward}')\n",
    "    rewards.append(total_reward)\n",
    "    #water_levels.append(mean_episode_level)\n",
    "    #input_flows.append(mean_episode_inflow)\n",
    "\n",
    "#fig, axes = plt.subplots(nrows = 1, ncols = 3)\n",
    "plt.figure(1)\n",
    "plt.plot(np.arange(len(rewards))*env.dt, rewards)\n",
    "#axes[1].plot(np.arange(len(water_levels))*env.dt, water_levels)\n",
    "#axes[2].plot(np.arange(len(input_flows))*env.dt, input_flows)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.0872, 4.6896, 4.0410])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "WaterTank.step() missing 1 required positional argument: 'valve_perc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[121], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Deploy the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m state_deploy \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m      3\u001b[0m done_deploy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Create a figure to plot the water level\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: WaterTank.step() missing 1 required positional argument: 'valve_perc'"
     ]
    }
   ],
   "source": [
    "# Deploy the model\n",
    "state_deploy = torch.tensor(env.step(0)[0], dtype=torch.float32)\n",
    "done_deploy = False\n",
    "\n",
    "# Create a figure to plot the water level\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "\n",
    "while not done_deploy:\n",
    "    q_values_deploy = model(state_deploy)\n",
    "    action_deploy = torch.argmax(q_values_deploy).item()\n",
    "\n",
    "    next_state_deploy, reward_deploy = env.step(action_deploy)\n",
    "\n",
    "    state_deploy = torch.tensor(next_state_deploy, dtype=torch.float32)\n",
    "\n",
    "    env.current_level += next_state_deploy[1]  # Update current level based on action taken\n",
    "\n",
    "    env_render_info = f\"Current Level: {env.current_level}, Reward: {reward_deploy}\"\n",
    "    \n",
    "    print(env_render_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
