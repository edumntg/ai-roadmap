{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '__version__' from 'tensorflow.keras' (c:\\Python311\\Lib\\site-packages\\keras\\api\\_v2\\keras\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Env\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspaces\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Discrete, Box, Dict\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DQNAgent\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolicy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BoltzmannQPolicy\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmemory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SequentialMemory\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\rl\\agents\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdqn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DQNAgent, NAFAgent, ContinuousDQNAgent\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mddpg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DDPGAgent\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CEMAgent\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\rl\\agents\\dqn.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Lambda, Input, Layer, Dense\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Agent\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolicy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EpsGreedyQPolicy, GreedyQPolicy\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\rl\\core.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m History\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      8\u001b[0m     CallbackList,\n\u001b[0;32m      9\u001b[0m     TestLogger,\n\u001b[0;32m     10\u001b[0m     TrainEpisodeLogger,\n\u001b[0;32m     11\u001b[0m     TrainIntervalLogger,\n\u001b[0;32m     12\u001b[0m     Visualizer\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mAgent\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Abstract base class for all implemented agents.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m    Each agent interacts with the environment (as defined by the `Env` class) by first observing the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;124;03m        processor (`Processor` instance): See [Processor](#processor) for details.\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\rl\\callbacks.py:8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m KERAS_VERSION\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callback \u001b[38;5;28;01mas\u001b[39;00m KerasCallback, CallbackList \u001b[38;5;28;01mas\u001b[39;00m KerasCallbackList\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneric_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Progbar\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name '__version__' from 'tensorflow.keras' (c:\\Python311\\Lib\\site-packages\\keras\\api\\_v2\\keras\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import gym\n",
    "import random\n",
    "\n",
    "from keras import __version__\n",
    "tf.keras.__version__ = __version__\n",
    "\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box, Dict\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\gym\\envs\\registration.py:555: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "states = env.observation_space.shape[0]\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 Score: 19.0\n",
      "Episode 2 Score: 15.0\n",
      "Episode 3 Score: 49.0\n",
      "Episode 4 Score: 28.0\n",
      "Episode 5 Score: 14.0\n",
      "Episode 6 Score: 18.0\n",
      "Episode 7 Score: 19.0\n",
      "Episode 8 Score: 12.0\n",
      "Episode 9 Score: 12.0\n",
      "Episode 10 Score: 16.0\n",
      "Episode 11 Score: 16.0\n",
      "Episode 12 Score: 11.0\n",
      "Episode 13 Score: 10.0\n",
      "Episode 14 Score: 17.0\n",
      "Episode 15 Score: 11.0\n",
      "Episode 16 Score: 18.0\n",
      "Episode 17 Score: 45.0\n",
      "Episode 18 Score: 18.0\n",
      "Episode 19 Score: 9.0\n",
      "Episode 20 Score: 26.0\n",
      "Episode 21 Score: 16.0\n",
      "Episode 22 Score: 12.0\n",
      "Episode 23 Score: 15.0\n",
      "Episode 24 Score: 11.0\n",
      "Episode 25 Score: 10.0\n",
      "Episode 26 Score: 23.0\n",
      "Episode 27 Score: 13.0\n",
      "Episode 28 Score: 17.0\n",
      "Episode 29 Score: 14.0\n",
      "Episode 30 Score: 25.0\n",
      "Episode 31 Score: 9.0\n",
      "Episode 32 Score: 15.0\n",
      "Episode 33 Score: 31.0\n",
      "Episode 34 Score: 52.0\n",
      "Episode 35 Score: 12.0\n",
      "Episode 36 Score: 19.0\n",
      "Episode 37 Score: 17.0\n",
      "Episode 38 Score: 53.0\n",
      "Episode 39 Score: 21.0\n",
      "Episode 40 Score: 23.0\n",
      "Episode 41 Score: 16.0\n",
      "Episode 42 Score: 24.0\n",
      "Episode 43 Score: 14.0\n",
      "Episode 44 Score: 31.0\n",
      "Episode 45 Score: 24.0\n",
      "Episode 46 Score: 18.0\n",
      "Episode 47 Score: 22.0\n",
      "Episode 48 Score: 50.0\n",
      "Episode 49 Score: 25.0\n",
      "Episode 50 Score: 12.0\n",
      "Episode 51 Score: 34.0\n",
      "Episode 52 Score: 17.0\n",
      "Episode 53 Score: 15.0\n",
      "Episode 54 Score: 13.0\n",
      "Episode 55 Score: 13.0\n",
      "Episode 56 Score: 16.0\n",
      "Episode 57 Score: 13.0\n",
      "Episode 58 Score: 38.0\n",
      "Episode 59 Score: 21.0\n",
      "Episode 60 Score: 24.0\n",
      "Episode 61 Score: 19.0\n",
      "Episode 62 Score: 21.0\n",
      "Episode 63 Score: 12.0\n",
      "Episode 64 Score: 12.0\n",
      "Episode 65 Score: 31.0\n",
      "Episode 66 Score: 11.0\n",
      "Episode 67 Score: 15.0\n",
      "Episode 68 Score: 43.0\n",
      "Episode 69 Score: 20.0\n",
      "Episode 70 Score: 30.0\n",
      "Episode 71 Score: 38.0\n",
      "Episode 72 Score: 19.0\n",
      "Episode 73 Score: 15.0\n",
      "Episode 74 Score: 11.0\n",
      "Episode 75 Score: 30.0\n",
      "Episode 76 Score: 25.0\n",
      "Episode 77 Score: 39.0\n",
      "Episode 78 Score: 14.0\n",
      "Episode 79 Score: 19.0\n",
      "Episode 80 Score: 17.0\n",
      "Episode 81 Score: 24.0\n",
      "Episode 82 Score: 21.0\n",
      "Episode 83 Score: 17.0\n",
      "Episode 84 Score: 18.0\n",
      "Episode 85 Score: 14.0\n",
      "Episode 86 Score: 11.0\n",
      "Episode 87 Score: 35.0\n",
      "Episode 88 Score: 19.0\n",
      "Episode 89 Score: 14.0\n",
      "Episode 90 Score: 26.0\n",
      "Episode 91 Score: 14.0\n",
      "Episode 92 Score: 30.0\n",
      "Episode 93 Score: 15.0\n",
      "Episode 94 Score: 13.0\n",
      "Episode 95 Score: 11.0\n",
      "Episode 96 Score: 34.0\n",
      "Episode 97 Score: 21.0\n",
      "Episode 98 Score: 12.0\n",
      "Episode 99 Score: 13.0\n",
      "Episode 100 Score: 48.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:211: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"CartPole-v0\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "episodes = 100\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = random.choice([0,1])\n",
    "        n_state, reward, done, truncated,info = env.step(action)\n",
    "        score += reward\n",
    "    print(\"Episode {} Score: {}\".format(episode, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the WaterTank environment\n",
    "class WaterTank(Env):\n",
    "    def __init__(self, setpoint = 10.0, dt = 1e-3, tf = 10):\n",
    "        \"\"\"\n",
    "        The tank is governed by the following first-order differential equation\n",
    "        \n",
    "        A(dH/dt) = bV - a*sqrt(H)\n",
    "\n",
    "        The states of the system are: H\n",
    "        The inputs of the system are: v\n",
    "        The outputs of the system are: H\n",
    "\n",
    "        H:  Height of water\n",
    "        V: voltage applied to the pump\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Possible actions\n",
    "        self.action_space = Discrete(5) # open, close for output flows, increase/decrease/stay for pump voltage\n",
    "        \n",
    "        # Water level temperature array\n",
    "        #self.observation_space = Box(low = np.array([1e-6]), high = np.array([200]))\n",
    "        self.observation_space = np.array([\n",
    "            Box(low = np.array([1e-6]), high = np.array([200])),\n",
    "            Box(low = np.array([1e-6]), high = np.array([500])),\n",
    "            Box(low = np.array([1e-6]), high = np.array([100]))\n",
    "        ])\n",
    "\n",
    "        # self.observation_space = Dict({\n",
    "        #     \"H\": Box(low = np.array([1e-6]), high = np.array([200])),\n",
    "        #     \"Q\": Box(low = np.array([1e-6]), high = np.array([500])),\n",
    "        #     \"V\": Box(low = np.array([1e-6]), high = np.array([100]))\n",
    "        # })\n",
    "\n",
    "        # Tank parameters\n",
    "        self.H0 = 100 + np.random.uniform(-100, 100) # initial level\n",
    "        self.A = 20 # cross sectional area of the tank\n",
    "\n",
    "        # Initialize current water level\n",
    "        self.H = self.H0 # meters\n",
    "\n",
    "        # Set desired level\n",
    "        self.setpoint = setpoint\n",
    "\n",
    "        # Input valve parameters\n",
    "        self.b = 5 # Constant related to the flow rate into the tank\n",
    "        self.V = 5 + np.random.uniform(-5, 10) # Initial voltage of the pump\n",
    "        self.P = self.V*self.b\n",
    "\n",
    "        # Output valve parameters\n",
    "        self.a = 2 # Constant related to the flow rate out of the tank\n",
    "        self.Q = self.a*math.sqrt(self.H) # Output flow depends of pressure which depends of water height\n",
    "\n",
    "        self.dt = dt # each iteration is a 10ms time-step\n",
    "        self.tf = tf\n",
    "\n",
    "        # Initial water level, inflow and outflow\n",
    "        self.state = self.H\n",
    "\n",
    "        self.t = 0\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "            action 0: Open outflow valve\n",
    "            action 1: Close outflow valve\n",
    "            action 2: Increase voltage for pump\n",
    "            action 3: Decrease voltage for pump\n",
    "            action 4: Stay in same voltage\n",
    "        \"\"\"\n",
    "        if action == 0: # Open outflow valve\n",
    "            self.Q = self.a*math.sqrt(self.H)\n",
    "        elif action == 1: # close outflow valve\n",
    "            self.Q = 0.0\n",
    "        elif action == 2: # Increase pump voltage\n",
    "            self.V += 0.5\n",
    "        elif action == 3: # Decrease pump voltage\n",
    "            self.V -= 0.5\n",
    "\n",
    "        # Update inflow\n",
    "        self.P = self.b*self.V\n",
    "\n",
    "        # Output flow\n",
    "        self.Q = self.a*math.sqrt(self.H)\n",
    "\n",
    "        # Update tank level using trapezoidal method\n",
    "        Hold = self.H\n",
    "        Hnew = Hold + (self.dt/2.0)*((self.P - self.Q)/self.A)\n",
    "        #self.H += ((self.P - self.Q)/self.A) * self.dt # Trapezoidal method\n",
    "        self.H = Hnew\n",
    "        if self.H <= 0:\n",
    "            self.H = 1e-6\n",
    "\n",
    "        # Compute reward as the negative abs difference between current level and setpoint\n",
    "        \n",
    "        reward = -abs(self.H - self.setpoint)  # Negative absolute difference as reward\n",
    "\n",
    "        # Update time\n",
    "        self.t += self.dt\n",
    "\n",
    "        done = self.t >= self.tf\n",
    "\n",
    "        info = {}\n",
    "        #self.state = self.H\n",
    "        self.state = np.array([self.H, self.Q, self.V])\n",
    "        return self.state, reward, done, info\n",
    "    \n",
    "    def render(self):\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        # Initialize current water level\n",
    "        self.H0 = 100 + np.random.uniform(-100, 100) # initial level\n",
    "        self.H = self.H0 # meters\n",
    "\n",
    "        # Input valve parameters\n",
    "        self.V = 5 + np.random.uniform(-5, 10) # Initial voltage of the pump\n",
    "        self.P = self.V*self.b\n",
    "\n",
    "        # Output valve parameters\n",
    "        self.Q = self.a*math.sqrt(self.H) # Output flow depends of pressure which depends of water height\n",
    "\n",
    "        reward = -abs(self.H - self.setpoint)  # Negative absolute difference as reward\n",
    "\n",
    "        #self.state = self.H\n",
    "        self.state = np.array([self.H, self.Q, self.V])\n",
    "\n",
    "        self.t = 0\n",
    "\n",
    "        return self.state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\gym\\spaces\\box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "env = WaterTank(\n",
    "    tf = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 Score: -336893.1790654605\n",
      "Episode 2 Score: -385184.6690240174\n",
      "Episode 3 Score: -544584.665061998\n",
      "Episode 4 Score: -34300.66411421761\n",
      "Episode 5 Score: -60013.70736119558\n",
      "Episode 6 Score: -387399.4643059907\n",
      "Episode 7 Score: -839261.6482947398\n",
      "Episode 8 Score: -380153.60191190935\n",
      "Episode 9 Score: -939425.7181825117\n",
      "Episode 10 Score: -360593.8153474734\n"
     ]
    }
   ],
   "source": [
    "episodes = 10\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done,info = env.step(action)\n",
    "        score += reward\n",
    "    print(\"Episode {} Score: {}\".format(episode, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Flatten(input_shape = (1, states)))\n",
    "    model.add(tf.keras.layers.Dense(24, activation = 'relu'))\n",
    "    model.add(tf.keras.layers.Dense(24, activation = 'relu'))\n",
    "    model.add(tf.keras.layers.Dense(actions, activation = 'linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 5\n",
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_2 (Flatten)         (None, 3)                 0         \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 24)                96        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n",
      " dense_48 (Dense)            (None, 24)                600       \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 5)                 125       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 821 (3.21 KB)\n",
      "Trainable params: 821 (3.21 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "states = env.observation_space.shape[0]\n",
    "actions = env.action_space.n\n",
    "print(states, actions)\n",
    "\n",
    "model = build_model(states, actions)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit = 50000, window_length = 1)\n",
    "    dqn = DQNAgent(\n",
    "        model = model,\n",
    "        memory = memory,\n",
    "        policy = policy,\n",
    "        nb_actions = actions,\n",
    "        nb_steps_warmup = 10,\n",
    "        target_model_update = 1e-2\n",
    "    )\n",
    "\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "    1/10000 [..............................] - ETA: 34:55 - reward: -131.1485"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "c:\\Python311\\Lib\\site-packages\\rl\\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 59s 6ms/step - reward: -68.0554\n",
      "2 episodes - episode_reward: -340276.758 [-606901.177, -73652.338] - loss: 1379.688 - mae: 2619.246 - mean_q: -3267.690\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 71s 7ms/step - reward: -159.8034\n",
      "2 episodes - episode_reward: -799017.203 [-802153.233, -795881.173] - loss: 4396.565 - mae: 5500.212 - mean_q: -6865.637\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 69s 7ms/step - reward: -111.4144\n",
      "2 episodes - episode_reward: -557071.849 [-786294.711, -327848.988] - loss: 11792.746 - mae: 8037.651 - mean_q: -10030.938\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 71s 7ms/step - reward: -32.3779\n",
      "2 episodes - episode_reward: -161889.507 [-318136.749, -5642.264] - loss: 12713.874 - mae: 7162.763 - mean_q: -8935.025\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 66s 7ms/step - reward: -57.0015\n",
      "done, took 335.924 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x23e990f32d0>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(tf.keras.optimizers.legacy.Adam(learning_rate = 1e-3), metrics = ['mae'])\n",
    "dqn.fit(env, nb_steps = 50000, visualize = False, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 100 episodes ...\n",
      "Episode 1: reward: -85890.951, steps: 5000\n",
      "Episode 2: reward: -470011.374, steps: 5000\n",
      "Episode 3: reward: -451272.060, steps: 5000\n",
      "Episode 4: reward: -480266.604, steps: 5000\n",
      "Episode 5: reward: -481224.517, steps: 5000\n",
      "Episode 6: reward: -442864.667, steps: 5000\n",
      "Episode 7: reward: -45584.604, steps: 5000\n",
      "Episode 8: reward: -490902.595, steps: 5000\n",
      "Episode 9: reward: -362091.592, steps: 5000\n",
      "Episode 10: reward: -448551.000, steps: 5000\n",
      "Episode 11: reward: -320157.492, steps: 5000\n",
      "Episode 12: reward: -47872.227, steps: 5000\n",
      "Episode 13: reward: -481396.881, steps: 5000\n",
      "Episode 14: reward: -509398.773, steps: 5000\n",
      "Episode 15: reward: -468718.254, steps: 5000\n",
      "Episode 16: reward: -321360.283, steps: 5000\n",
      "Episode 17: reward: -302878.631, steps: 5000\n",
      "Episode 18: reward: -29094.911, steps: 5000\n",
      "Episode 19: reward: -220312.110, steps: 5000\n",
      "Episode 20: reward: -77147.905, steps: 5000\n",
      "Episode 21: reward: -186861.263, steps: 5000\n",
      "Episode 22: reward: -47195.644, steps: 5000\n",
      "Episode 23: reward: -248015.655, steps: 5000\n",
      "Episode 24: reward: -442834.484, steps: 5000\n",
      "Episode 25: reward: -363666.956, steps: 5000\n",
      "Episode 26: reward: -150167.032, steps: 5000\n",
      "Episode 27: reward: -202669.998, steps: 5000\n",
      "Episode 28: reward: -451873.425, steps: 5000\n",
      "Episode 29: reward: -162810.832, steps: 5000\n",
      "Episode 30: reward: -480693.903, steps: 5000\n",
      "Episode 31: reward: -112260.379, steps: 5000\n",
      "Episode 32: reward: -459966.732, steps: 5000\n",
      "Episode 33: reward: -388723.684, steps: 5000\n",
      "Episode 34: reward: -425239.961, steps: 5000\n",
      "Episode 35: reward: -373191.838, steps: 5000\n",
      "Episode 36: reward: -459148.043, steps: 5000\n",
      "Episode 37: reward: -45588.813, steps: 5000\n",
      "Episode 38: reward: -471084.462, steps: 5000\n",
      "Episode 39: reward: -522619.504, steps: 5000\n",
      "Episode 40: reward: -467911.932, steps: 5000\n",
      "Episode 41: reward: -297505.385, steps: 5000\n",
      "Episode 42: reward: -67072.023, steps: 5000\n",
      "Episode 43: reward: -442220.955, steps: 5000\n",
      "Episode 44: reward: -58871.091, steps: 5000\n",
      "Episode 45: reward: -295473.865, steps: 5000\n",
      "Episode 46: reward: -495229.613, steps: 5000\n",
      "Episode 47: reward: -508191.562, steps: 5000\n",
      "Episode 48: reward: -483275.236, steps: 5000\n",
      "Episode 49: reward: -499569.742, steps: 5000\n",
      "Episode 50: reward: -477835.102, steps: 5000\n",
      "Episode 51: reward: -490119.704, steps: 5000\n",
      "Episode 52: reward: -121585.957, steps: 5000\n",
      "Episode 53: reward: -357486.980, steps: 5000\n",
      "Episode 54: reward: -450268.837, steps: 5000\n",
      "Episode 55: reward: -468051.309, steps: 5000\n",
      "Episode 56: reward: -506557.140, steps: 5000\n",
      "Episode 57: reward: -515484.646, steps: 5000\n",
      "Episode 58: reward: -185824.606, steps: 5000\n",
      "Episode 59: reward: -432319.983, steps: 5000\n",
      "Episode 60: reward: -453178.825, steps: 5000\n",
      "Episode 61: reward: -499412.311, steps: 5000\n",
      "Episode 62: reward: -467469.891, steps: 5000\n",
      "Episode 63: reward: -238359.128, steps: 5000\n",
      "Episode 64: reward: -95065.204, steps: 5000\n",
      "Episode 65: reward: -474668.311, steps: 5000\n",
      "Episode 66: reward: -49770.727, steps: 5000\n",
      "Episode 67: reward: -451038.704, steps: 5000\n",
      "Episode 68: reward: -379892.189, steps: 5000\n",
      "Episode 69: reward: -450610.460, steps: 5000\n",
      "Episode 70: reward: -45621.676, steps: 5000\n",
      "Episode 71: reward: -430561.145, steps: 5000\n",
      "Episode 72: reward: -478134.299, steps: 5000\n",
      "Episode 73: reward: -30398.179, steps: 5000\n",
      "Episode 74: reward: -461711.557, steps: 5000\n",
      "Episode 75: reward: -46002.448, steps: 5000\n",
      "Episode 76: reward: -46967.078, steps: 5000\n",
      "Episode 77: reward: -448102.172, steps: 5000\n",
      "Episode 78: reward: -210003.010, steps: 5000\n",
      "Episode 79: reward: -625320.756, steps: 5000\n",
      "Episode 80: reward: -505314.992, steps: 5000\n",
      "Episode 81: reward: -500866.303, steps: 5000\n",
      "Episode 82: reward: -231345.862, steps: 5000\n",
      "Episode 83: reward: -498568.344, steps: 5000\n",
      "Episode 84: reward: -155716.767, steps: 5000\n",
      "Episode 85: reward: -478222.744, steps: 5000\n",
      "Episode 86: reward: -306632.931, steps: 5000\n",
      "Episode 87: reward: -291708.562, steps: 5000\n",
      "Episode 88: reward: -304805.265, steps: 5000\n",
      "Episode 89: reward: -541945.498, steps: 5000\n",
      "Episode 90: reward: -193467.502, steps: 5000\n",
      "Episode 91: reward: -259929.892, steps: 5000\n",
      "Episode 92: reward: -48347.313, steps: 5000\n",
      "Episode 93: reward: -111376.676, steps: 5000\n",
      "Episode 94: reward: -455469.454, steps: 5000\n",
      "Episode 95: reward: -434784.701, steps: 5000\n",
      "Episode 96: reward: -143080.117, steps: 5000\n",
      "Episode 97: reward: -226882.801, steps: 5000\n",
      "Episode 98: reward: -135284.369, steps: 5000\n",
      "Episode 99: reward: -292380.434, steps: 5000\n",
      "Episode 100: reward: -440880.135, steps: 5000\n",
      "-330917.6444087177\n"
     ]
    }
   ],
   "source": [
    "scores = dqn.test(env, nb_episodes = 100, visualize = False)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
